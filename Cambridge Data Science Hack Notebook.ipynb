{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambridge Data Science Hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>You do not have to work in this Notebook, but a read-through may provide some helpful hints.</h3>\n",
    "<h4>Introduction</h4>\n",
    "<p>This challenge focuses on extracting meaning from text. Use cases in a finance company could include:</p>\n",
    "<ul>\n",
    " <li>Automatic chatbots</li>\n",
    " <li>Classifying customer complaints and communications</li>\n",
    " <li>Automated underwriting from medical records</li>\n",
    " <li>Automated claims handling from accident reports and call transcripts</li>\n",
    " </ul>\n",
    " \n",
    "<p>Here we will use data from government e-petitions to explore two common uses of text in Data Science:</p>\n",
    "<ul>\n",
    " <li>Predictive Modelling</li>\n",
    " <li>Topic Classification</li>\n",
    " </ul>\n",
    " \n",
    "<br></br>\n",
    "<p>You may wish to work in this Notebook for your solution. But feel free to use any coding language, any approach, any GUI.</p>\n",
    " <br></br>\n",
    "<p>_Contains Parliamentary information licensed under the Open Parliament Licence v3.0._\n",
    "https://www.parliament.uk/site-information/copyright/open-parliament-licence/</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing all the packages we might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import plotly.express as px\n",
    "from sklearn.utils import resample\n",
    "from operator import itemgetter\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "We have training and holdout data. You'll need to use the training data to create a model and then use your model to label the holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.json', 'rb') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "with open('holdout_data.json', 'rb') as f:\n",
    "    holdout_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first petition in our training set, and have a look at how many petitions and signatures we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': {'_value': 'MPs should attend all debates, not merely turn up and vote or strike pairing deals. With other commitments, a five day Commons is not workable for MPs: I suggest three full days (9am to 6pm minimum), with one or two days for Committees, leaving at least one day for constituency work.'}, 'created': {'_value': '2016-03-15T15:56:53.752Z', '_datatype': 'dateTime'}, 'label': {'_value': 'Reform the Commons: Three days full time with compulsory attendance for all MPs.'}, 'numberOfSignatures': 27, 'status': 'closed'}\n",
      "\n",
      "Number of petitions in training data: 12387\n",
      "\n",
      "Mean number of signatures: 3777\n",
      "\n",
      "Median number of signatures: 58\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])\n",
    "\n",
    "print(\"\\nNumber of petitions in training data: {}\".format(len(training_data)))\n",
    "print(\"\\nMean number of signatures: {}\".format(int(np.mean([p['numberOfSignatures'] for p in training_data]))))\n",
    "print(\"\\nMedian number of signatures: {}\".format(int(np.median([p['numberOfSignatures'] for p in training_data]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the petition text is stored under two keys - the _value_ of _label_ gives the petition title, and the _value_ of _abstract_ provides a longer description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>Cleaning the Text</h4>\n",
    "<p>Usually in a Data Science problem we would start with Exploratory Data Analysis (EDA). But this is unstructured text - we can't calculate simple statistics or plot interesting histograms until we have turned this text into numbers. Before thinking about making a model, <b>structure and clean the text</b>.</p>\n",
    "<br></br>\n",
    "<p>You could consider:</p>\n",
    "<ul>\n",
    "    <li>Tokenizing</li>\n",
    " <li>Lemmatizing or Stemming</li>\n",
    " <li>Filtering</li>\n",
    " <li>Calculating TF-IDF</li>\n",
    " </ul>\n",
    " \n",
    "<p>Packages of use may include:</p>\n",
    "<ul>\n",
    " <li>NLTK</li>\n",
    " <li>Gensim</li>\n",
    " <li>Sklearn</li>\n",
    " </ul>\n",
    " \n",
    "<p>Any blog post about any form of text modelling will begin with a section on text preprocessing. Some examples are here:</p>\n",
    "<ul>\n",
    " <li>https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb</li>\n",
    " <li>https://www.machinelearningplus.com/nlp/gensim-tutorial/#8howtocreatethetfidfmatrixcorpusingensim</li>\n",
    " <li>https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089</li>\n",
    " <li>https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925</li>\n",
    " </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Challenge Part I: Predict Whether a Petition Surpasses 50 Signatures</h4>\n",
    "<p>You need to create a model to <b>predict whether a petition has surpassed 50 signatures, using only the petition text as input</b>.</p>\n",
    "<br></br>\n",
    "<p>Your first task will probably be feature generation. You may wish to consider:</p>\n",
    "<ul>\n",
    " <li>Word counts</li>\n",
    " <li>Word frequencies (and TF-IDF)</li>\n",
    " <li>Word embedding</li>\n",
    " <li>Custom rules</li>\n",
    " </ul>\n",
    " \n",
    "\n",
    "<p>This is a supervised learning task using text - multiple blog posts cover walkthroughs for various purposes (e.g. sentiment analysis). Some potential resources are here:</p>\n",
    "<ul>\n",
    " <li>https://medium.com/@annabiancajones/sentiment-analysis-on-reviews-feature-extraction-and-logistic-regression-43a29635cc81</li>\n",
    " <li>https://www.kaggle.com/arunava21/word2vec-and-random-forest-classification</li>\n",
    " </ul>\n",
    "<br></br>\n",
    "<p>Once you have a model, <b>predict whether each petition in the holdout set will surpass 50 signatures</b>. Your predictions will be assessed against the truth using the F1 score. F1 accounts for both Precision (of all the petitions you predict to surpass 50 signatures, how often were you correct) and Recall (of all the petitions which surpass 50 signatures, how many do you correctly identify).</p>\n",
    "<br></br>\n",
    "<p>Your submission for Part I should be a CSV list of 3,000 Booleans to represent your predictions for the holdout set - True if you think a petition will surpass 50 signatures, False otherwise.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>Challenge Part II: Topic Modelling</h4>\n",
    "<p>In many problems it is useful to cluster texts together which seem to talk about the same topic. You may wish to understand what customers tend to complain about, what news articles tend to be about or what reviews tend to talk about. For this task, <b>you need to automatically group the petitions into topics.</b>.</p>\n",
    "<br></br>\n",
    "<p>On the hack day you will show us your topic classification and explain how you have decided - quantiatively or qualitatively - on your end result.</p>\n",
    "<br></br>\n",
    "<p>You may wish to consider:</p>\n",
    "<ul>\n",
    " <li>LDA</li>\n",
    " <li>Clustering</li>\n",
    " </ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
